{
  "engine_name": "llama-factory",
  "model_format": "safetensors",
  "enabled": 1,
  "container_port": 8000,
  "engine_images": [
    {
      "compute_type": "gpu",
      "image": "llama-factory:0.9.3",
      "driver_version": "12.1",
      "engine_version": "0.9.3"
    },
    {
      "compute_type": "dcu",
      "image": "llama-factory:0.9.3-dtk25.04",
      "driver_version": "25.04",
      "engine_version": "0.9.3"
    }
  ],
  "supported_archs": [
    "BaiChuanForCausalLM",
    "BloomForCausalLM",
    "ChatGLMModel",
    "CohereForCausalLM",
    "DeepseekV2ForCausalLM",
    "DeepseekV3ForCausalLM",
    "FalconForCausalLM",
    "Gemma2ForCausalLM",
    "GemmaForCausalLM",
    "InternLM2ForCausalLM",
    "InternVLChatModel",
    "InternLM2ForRewardModel",
    "LlamaForCausalLM",
    "LlavaForConditionalGeneration",
    "LlavaLlamaForCausalLM",
    "LlavaNextForConditionalGeneration",
    "MistralForCausalLM",
    "MixtralForCausalLM",
    "OlmoForCausalLM",
    "PaliGemmaForConditionalGeneration",
    "Phi3ForCausalLM",
    "PhiForCausalLM",
    "QWenLMHeadModel",
    "Qwen2ForCausalLM",
    "Qwen2MoeForCausalLM",
    "Qwen3ForCausalLM", 
    "Qwen3MoeForCausalLM",
    "Starcoder2ForCausalLM",
    "XverseForCausalLM",
    "YuanForCausalLM"
  ]
}
