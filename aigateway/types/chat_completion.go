package types

import "github.com/openai/openai-go"

// Represents a streamed chunk of a chat completion response returned by model,
// based on the provided input.
type ChatCompletionChunk struct {
	// A unique identifier for the chat completion. Each chunk has the same ID.
	ID string `json:"id"`
	// A list of chat completion choices. Can contain more than one elements if `n` is
	// greater than 1. Can also be empty for the last chunk if you set
	// `stream_options: {"include_usage": true}`.
	Choices []ChatCompletionChunkChoice `json:"choices"`
	// The Unix timestamp (in seconds) of when the chat completion was created. Each
	// chunk has the same timestamp.
	Created int64 `json:"created"`
	// The model to generate the completion.
	Model string `json:"model"`
	// The object type, which is always `chat.completion.chunk`.
	Object openai.ChatCompletionChunkObject `json:"object"`
	// The service tier used for processing the request.
	ServiceTier openai.ChatCompletionChunkServiceTier `json:"service_tier"`
	// This fingerprint represents the backend configuration that the model runs with.
	// Can be used in conjunction with the `seed` request parameter to understand when
	// backend changes have been made that might impact determinism.
	SystemFingerprint string `json:"system_fingerprint"`
	// An optional field that will only be present when you set
	// `stream_options: {"include_usage": true}` in your request. When present, it
	// contains a null value except for the last chunk which contains the token usage
	// statistics for the entire request.
	Usage openai.CompletionUsage `json:"usage"`
}

type ChatCompletionChunkChoice struct {
	// A chat completion delta generated by streamed model responses.
	Delta ChatCompletionChunkChoicesDelta `json:"delta"`
	// The reason the model stopped generating tokens. This will be `stop` if the model
	// hit a natural stop point or a provided stop sequence, `length` if the maximum
	// number of tokens specified in the request was reached, `content_filter` if
	// content was omitted due to a flag from our content filters, `tool_calls` if the
	// model called a tool, or `function_call` (deprecated) if the model called a
	// function.
	FinishReason openai.ChatCompletionChunkChoicesFinishReason `json:"finish_reason"`
	// The index of the choice in the list of choices.
	Index int64 `json:"index"`
	// Log probability information for the choice.
	Logprobs openai.ChatCompletionChunkChoicesLogprobs `json:"logprobs"`
}

// A chat completion delta generated by streamed model responses.
type ChatCompletionChunkChoicesDelta struct {
	// The contents of the chunk message.
	Content          string `json:"content"`
	ReasoningContent string `json:"reasoning_content"`
	// The refusal message generated by the model.
	Refusal string `json:"refusal"`
	// The role of the author of this message.
	Role      openai.ChatCompletionChunkChoicesDeltaRole       `json:"role"`
	ToolCalls []openai.ChatCompletionChunkChoicesDeltaToolCall `json:"tool_calls"`
}

type ChatCompletion struct {
	openai.ChatCompletion
}
